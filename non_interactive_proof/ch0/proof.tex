\begin{codeblock}
  \textbf{Experiment $E^{P^{(g)}, C^{(.)(.)}, hash}(\pi_1, \dots, \pi_k)$} \\
  Solving $k$-wise direct product of DWVP with respect to the set $P_{hash}$
  \medskip

  \hrule

  \medskip
  \textbf{Oracle:} Problem poser for k-wise direct product $P^{(g)}$ \\
  \IndI A solver circuit for $k$-wise direct product $C^{(\cdot, \cdot)}$ \\
  \IndI A function $hash: Q \leftarrow \{0, \dots, 2(h+v) - 1\}$\\
  \textbf{Input:} Random bitstring $(\pi_1, \dots, \pi_k) \in \{0,1\}^{kl}$\\

  \medskip\hrule\medskip

  $\pi^{(k)} := \left(\pi_1, \dots, \pi_k \right)$\\
  $(x^{k}, \Gamma_V^{(g)}, \Gamma_H^{(k)}) := P^{(g)}(\pi^{k})$\\
  Run $C^{\Gamma_V^{(g)}, \Gamma_H^{(k)}} (x^{(k)})$ \\
  \IndI Let $(q_j,y_j^{(k)})$ be the first successful verification query if $C^{\Gamma_V^{(g)}, \Gamma_H^{(k)}}$ succeeds or \\
  \IndI an arbitrary verification query when it fails.\\
  \textbf{If} $(\forall i < j :  q_i \notin P_{hash} )$ and $q_j \in P_{hash}$ and $\Gamma_V^{(g)}(q_j, y_j^{(k)}) = 1$ \\
  \IndI \textbf{return} 1\\
  \textbf{else}\\
  \IndI \textbf{return} 0\\
\end{codeblock}
%
A canonical success is a situation when a solver $C$ for fixed $hash$ and $P^{(1)}$ succeeds in a random experiment $E$.
%
\begin{codeblock}
  \textbf{Random experiment $F^{P^{(1)}, D, hash}(\pi)$} \\
  Solving a single DWVP with respect to the set $P_{hash}$
  \medskip

  \hrule

  \medskip

  \textbf{Oracle:}
  A dynamic weakly verifiable puzzle $P^{(1)}$ \\
  \IndI A solver circuit for a single DWVP $D$ \\
  \IndI A function $hash: Q \rightarrow \{0,1,\dots, 2(h+v)-1\}$ \\
  \textbf{Input:} Random bitstring $\pi \in \{0,1\}^l$
  %TODO length of the bitstring maybe it is fixed as it is input to P^{(1)} ? like l see the end of the paper by Imaginazzo.
  \medskip\hrule\medskip

  $(x, \Gamma_v, \Gamma_H) := P^{(1)}(\pi)$ \\
  Run $D^{\Gamma_V, \Gamma_H}(x)$ \\
  \IndI Let $(\widetilde{q_j},\widetilde{r_j})$ be the first successful verification query if $D^{\Gamma_V, \Gamma_H}(x)$ succeeds or \\
  \IndI an arbitrary verification query when it fails. \\
  \If $(\forall i < j :  q_i \notin  P_{hash} )$ and $q_j \in P_{hash}$ and $\Gamma_V(q_j) = 1$ \then \\
  \IndI \return 1 \\
  \textbf{else}\\
  \IndI \return 0\\

\end{codeblock}
%
%
\begin{lemma}
  \label{lemma:sec_amp_for_p_hash}
  \textbf{Security amplification of a dynamic weakly verifiable puzzle with respect to set $P_{hash}$.} \\
  For a fixed dynamic weakly verifiable puzzle $P^{(1)}$ there exists an algorithm\\
  $Gen(C, g, \varepsilon, \delta, n, v, h, hash)$, which takes as input a circuit $C$, a monotone function $g$,
  a function $hash : Q \rightarrow \{0, \dots, 2(h+v)-1\}$, parameters $\varepsilon, \delta, n$,
  number of verification $v$, and hint $h$ queries asked by $C$, and outputs a circuit $D$
  such that following holds: \\
  If $C$ is such that \\
  \begin{align*}
    \underset{(\pi_1, \dots, \pi_k)}{\Pr}[E^{P^{(g)}, C, Hash}(\pi_1, \dots, \pi_k)=1] \geq \underset{\mu \leftarrow \mu_\delta^k}{\Pr}[g(\mu) = 1] + \varepsilon
  \end{align*}
  then $D$ satisfies almost surely
  \begin{align*}
    \underset{\pi}{\Pr}[F^{P^{(1)},D, Hash}(\pi) = 1] \geq (\delta + \frac{\varepsilon}{6k})
  \end{align*}
  and $Size(D) \leq Size(C)\frac{6k}{\varepsilon}$ and $Time(Gen) = poly(k, \frac{1}{\varepsilon}, n, v, h)$.
\end{lemma}

\begin{todo}
  \textbf{TODO:} The circuit should return the solutions to puzzles. Then we just need to call circuit $ \Gamma_v $ to eval. it.
  But there should be an assumption that the circuit always returns a tuple in $P_{hash}$ and does not ask hint or verification queries
  on this tuple.
\end{todo}

\begin{codeblock}
  \textbf{Circuit $\widetilde{C}^{\Gamma_V^{(g)}, \Gamma_H^{(g)}, hash, C} (x_1, \dots, x_k)$} \\
  Circuit $\widetilde{C}$ has good canonical success probability.
  \medskip

  \hrule

  \medskip

  \textbf{Oracle:} $\Gamma_V^{(g)}, \Gamma_H^{(g)}, hash, C$ \\
  \textbf{Input:} k-wise direct product of puzzles $(x_1, \dots, x_k)$

  \medskip\hrule\medskip
  Run $C^{(\cdot,\cdot)}(x_1, \dots, x_k)$ \\
  \IndI \If $C$ asks a hint query $q$ \then\\
  \IndII \If $q \in P_{hash}$ \then\\
  \IndIII \return $\bot$\\
  \IndII \textbf{else}\\
  \IndIII answer the hint query with $\Gamma_H^{(k)}(q)$\\
  \\
  \IndI \textbf{If} $C$ asks a verification query $(q, y_1, \dots, y_k)$ \textbf{then} \\
  \IndII \textbf{If} $q \in P_{hash}$ \textbf{then} \\
  \IndIII \text{ask the verification query} $(q, y_1, \dots, y_k)$ \\
  \IndIII \textbf{stop the execution} \\
  \IndII \textbf{else} \\
  \IndIII answer verification query with 0 \\
  \textbf{return} $\bot$
\end{codeblock}
The key difference between circuits $C$ and $\widetilde{C}$
is that if $\widetilde{C}$ asks a verification query $(q, y_1, \dots, y_k)$ then $q \in P_{hash}$.
This means that if $\widetilde{C}$ succeeds then it also succeeds canonically.

\begin{lemma}
  For fixed $P^{(g)}$ it is true that
  \begin{align*}
  \underset{(\pi_1, \dots, \pi_k)}{\Pr}[E^{P^{(g)}, C, Hash}(\pi_1, \dots, \pi_k) = 1] \leq \underset{(\pi_1, \dots, \pi_k)}{\Pr}[\Gamma_V^{(g)} (\widetilde{C}^{\Gamma_V^{(g)}, \Gamma_H^{(g)}, Hash}(\pi_1, \dots, \pi_k)) = 1].
  \end{align*}
\end{lemma}

\begin{proof}
We fix the randomness $(\pi_1, \dots, \pi_k)$ used in the random experiment $E$.
Let $x^{(k)} = (x_1, \dots, x_k)$ be a set of puzzles generated in the random experiment $E$ for the randomness $(\pi_1, \dots, \pi_k)$.
If $C$ succeeds canonically for the set of puzzles $x^{(k)}$, then also circuit $\widetilde{C}$ that runs $C$ on the same set of puzzles succeeds.
Using the definition of conditional expectation, we conclude that
\begin{align*}
  \underset{}{\Pr}[E^{P^{(g)}, C, hash}(\pi^{(k)}) = 1] &=
  \sum_{\pi^{(k)} \in \{0,1\}^{kl}}\underset{}{\Pr}[E^{P^{(g)}, C, hash}(\widetilde{\pi}^{(k)}) = 1 | \pi^{(k)} = \widetilde{\pi}^{(k)}] \underset{}{\Pr}[\pi^{(k)} = \widetilde{\pi}^{(k)}] \\
  &\leq
  \sum_{\pi^{(k)} \in \{0,1\}^{kl}}\underset{}{\Pr}[E^{P^{(g)}, \widetilde{C}, hash}(\widetilde{\pi}^{(k)}) = 1 | \pi^{(k)} = \widetilde{\pi}^{(k)}] \underset{}{\Pr}[\pi^{(k)} = \widetilde{\pi}^{(k)}] \\
  &= \underset{}{\Pr}[E^{P^{(g)}, \widetilde{C}, hash}(\pi^{(k)}) = 1]
\end{align*}

\end{proof}

\begin{codeblock}
  \textbf{Algorithm $Gen(\widetilde{C},g,\epsilon,\delta,n)$}
  \medskip

  \hrule

  \medskip

  \textbf{Oracle:} $\widetilde{C}, g$ \\
  \textbf{Input:}  $\epsilon, \delta, n$\\
  \textbf{Output:} A circuit $D$

  \medskip\hrule\medskip
  \If \text{the number of puzzles to solve equals one} \then \\
  \IndI \return $\widetilde{C}$ \\ \\
  \textbf{For} $i:=1$ to $\frac{6k}{\epsilon}\log(n)$ \\
  \IndI $\pi^* \leftarrow \{0,1\}^{l}$\\
  \IndI $\widetilde{S}_{\pi^*,0} := EvaluateSurplus(\pi^*, 0)$\\
  \IndI $\widetilde{S}_{\pi^*,1} := EvaluateSurplus(\pi^*, 1)$\\
  \IndI \textbf{If} $\widetilde{S}_{\pi^*,0} \geq (1 - \frac{3}{4k}) \epsilon$ or $\widetilde{S}_{\pi^*,1} \geq (1 - \frac{3}{4k}) \epsilon$ \\
  \IndII $\widetilde{C}' := \widetilde{C}$ with the first input fixed on $\pi^*$\\
  \IndII\textbf{return} $Gen(\widetilde{C}', g, \epsilon, \delta, n)$ \\
  // all estimates are lower than $(1-\frac{3}{4k})\varepsilon$\\
  \return $D^{\widetilde{C}}$ \\
  \\
  \textbf{EvaluateSurplus}($\pi^*, b$) \\
  \IndI \textbf{For} $i:=1$ to $N_k$ \\
  \IndII $(\pi_2, \dots, \pi_k) \xleftarrow{\$} \{0,1\}^{(k-1)l}$\\
  \IndII $(c_1, \dots, c_k) := EvalutePuzzles(\pi^*, \pi_2, \dots, \pi_k)$\\
  \IndII $\widetilde{S}_{\pi^*,b}^i := g(b, c_2, \dots, c_k) - \underset{(u_2, \dots, u_k)}{\Pr}[g(b, u_2, \dots, u_k) = 1] $\\
  \IndI \textbf{return} $\frac{1}{N_k} \sum_{i=1}^{N_k} \widetilde{S}_{\pi^*,b}^i$\\
  \\
  \textbf{EvalutePuzzles}($\pi^{(k)}$)\\
  \IndI $(x^{(k)}, \Gamma_V^{(g)}, \Gamma_H^{(k)}) := P^{(g)}(\pi^{(k)})$ \\
  \IndI \textbf{For} $i:=1$ to $k$\\
  \IndII $(x_i, \Gamma_V^{i}, \Gamma_H^{i}) := P^{(1)}(\pi_i)$\\
  \IndI $(q,y^{k}) := \widetilde{C}^{\Gamma_V^{(g)}, \Gamma_H^{(k)}}(x_1, x_2, \dots, x_k)$\\
  \IndI \textbf{For} $i:=1$ to $k$\\
  \IndII $c_i := \Gamma_v^{i}(q, y_i)$\\
  \IndI \textbf{return} $(c_1, \dots, c_k)$\\
\end{codeblock}
%
%
%
\begin{todo}
  \textbf{TODO:} Circuit $\widetilde{C}$ gets as input puzzle find a nice way to genereate the puzzles as it is used in many places in the code.
   Also make EvalutePuzzles more general maybe it should take $\widetilde{C}$ as input?
\end{todo}
\begin{codeblock}
  \textbf{Circuit $D^{\widetilde{C}}$}
  \medskip

  \hrule

  \medskip

  \textbf{Oracle:}  $\widetilde{C}, P^{(1)}$\\
  \textbf{Input:}  puzzle $x^*$, a random bitstring $r \in \{0,1\}^{*}$

  \medskip\hrule\medskip
  \textbf{For} $i:=1$ to $\frac{6k}{\epsilon} \log(\frac{6k}{\epsilon})$\\
  \IndI $\pi^{(k)} \leftarrow \{0,1\}^{kl}$ //read $k \cdot l$ bits from $r$  \\
  \IndI $(c_1, \dots, c_k) := EvaluatePuzzles(\pi^{(k)})$\\
  \IndI \textbf{If} $g(1,c_2, \dots, c_k) = 1$ and $g(0,c_2, \dots, c_k) = 0$\\
  \IndII $(q, y_1, \dots, y_k) := \widetilde{C}(x^*, x_2, \dots, x_k)$\\
  \IndII \textbf{return} $y_1$\\
  \textbf{return} $\bot$ \\

\end{codeblock}
%
% The algorithm $Gen$ recursively builds the circuit that have high success probability in solving a dynamic weakly verifiable puzzle.
% When algorithm recurses it fixes a puzzle on the first position on the input of circuit $\widetilde{C}$ which yields a new circuit $\widetilde{C}'$.
% It happens only in the situation when for some fixed $\pi^*$ circuit $\widetilde{C}$ performs good on the remaining $k-1$ puzzles.
%
For $k=1$ function $g(b)$ is either identity or a constant function.
If $g$ is identity then the success probability of $\widetilde{C}$ is as least $\delta + \epsilon$
and $\widetilde{C}$ can be directly used to solve a puzzle. If the function $g$ is constant the statement is vacuously true.

Let $(q, y_1, \dots, y_k)$ denote the output of $\widetilde{C}$.
Additionally, let us denote by $c_i = \Gamma_V(q, y_i)$ whether $(q,y_i)$ is a correct solution for a single puzzle.
We define surplus as the following quantity:
\begin{align}
  \label{eq:s_pi_b}
S_{\pi^*, b} = \underset{\pi^{(k)}}{\Pr}[g(b, c_2, \dots, c_k) = 1] - \underset{\mu^{(k)}}{\Pr}[g(b, u_2, \dots, u_k) = 1]
\end{align}
The surplus $S_{\pi^*, b}$ tells us how good the algorithm $\widetilde{C}$ performs when the first puzzle is fixed, and value of $c_1$ is neglected.
The procedure \textbf{EvaluateSurplus} returns the estimate for $\widetilde{S}_{\pi^*, b}$.
All puzzles used during obtaining the estimate are generated by $\textbf{EvaluatePuzzles}$.
Therefore, it is possible to provide answers for all hint and verification queries.
The returned estimate $\widetilde{S}_{\pi^*,b}$ that differs from $S_{\pi*, b}$
by at most $\frac{\epsilon}{4k}$ almost surely.
Therefore, if $\widetilde{S}_{\pi^*,b} \geq (1-\frac{3}{4k})\epsilon$ then
with high probability $S_{\pi*,b} \geq (1-\frac{1}{k})\epsilon$.
In this case we use a new monotone binary function $g'(b_2, \dots, b_k) := g(b, b_2, \dots, b_k)$, and fix the first puzzle of $\widetilde{C}$ for the one generated
by using the randomness $\pi^*$. The new circuit satisfies the conditions of Lemma \ref{lemma:sec_amp_for_p_hash} which means that we can
use algorithm $Gen$ for the new circuit $\widetilde{C}$ and monotone function $g'$.

If all estimates are less than $(1-\frac{1}{4k})\epsilon$, then intuitively $\widetilde{C}$
does not perform much better on the remaining $k-1$ puzzles than an algorithm that solves each puzzle independent with probability $\delta$.
However, from the assumption we know that on all $k$ puzzles $\widetilde{C}$ has high success probability.
It means that in this case the first puzzle has to be correctly solved with substantial probability.

\begin{todo}
  \textbf{TODO:} Explain the intuition why it may happen that we still can fail in the case of circuit $\widetilde{D}$.
\end{todo}

We have to show that the success probability when $Gen$ does not recurse is substantial.
We fix a randomness $\pi^*$ and thus also a puzzle $x^*$. For this fixed puzzle using (\ref{eq:s_pi_b}) we get
\begin{align}
\label{eq:diff_s01}
  &\underset{\mu_{\delta}^k}{\Pr}[g(1, \mu_2, \dots, \mu_k)=1] - \underset{\mu_{\delta}^k}{\Pr}[g(0, \mu_2, \dots, \mu_k)=1] = \notag\\
&\IndI  \underset{\pi^{(k)}}{\Pr}[g(1, c_2, \dots, c_k) =1 \mid \pi_1 = \pi^*] - \underset{\pi^k}{\Pr}[g(0, c_2, \dots, c_k) = 1 \mid \pi_1 = \pi^*] - (S_{\pi^*,1} - S_{\pi^*,0})
\end{align}
\begin{todo}
  \textbf{TODO:} Better explain why we can write $\Pr(g() = 1 \land g() = 0)$ as the equivalence for the difference.
\end{todo}
From the monotonicity of $g$ we know that for any set of tuples $(b_1, \dots, b_k)$
and sets $G_0 = \{ (b_1, b_2, \dots, b_k): g(0, b_2, \dots, b_k) = 1\}$, $ G_1 = \{(b_1, b_2, \dots, b_k) : g(1, b_2, \dots, b_k) = 1 \}$
we have $G_0 \subseteq G_1$. Hence, we can write (\ref{eq:diff_s01}):
\begin{align}
  \label{eq:diff_s01_next}
  &\underset{\mu_{\delta}^k}{\Pr}[g(1, \mu_2, \dots, \mu_k) = 1 \land g(0, \mu_2, \dots, \mu_k) = 0] = \notag\\
&\IndI  \underset{\pi^{(k)}}{\Pr}[g(1, c_2, \dots, c_k) = 1 \land g(0, c_2, \dots, c_k) = 0 \mid \pi_1 = \pi^*] - (S_{\pi^*,1} - S_{\pi^*,0}).
\end{align}
Let $G_{\mu^{(k)}}$ denote the event $g(1, \mu_2, \dots, \mu_k) = 1 \land g(0, \mu_2, \dots, \mu_k) = 0$, and correspondingly
$G_{\pi^{(k)}} := g(1, \pi_2, \dots, \pi_k) = 1 \land g(0, \pi_2, \dots, \pi_k) = 0$.
Then multiplying and dividing $\underset{}{\Pr}[\Gamma_v^{(g)}(D(x^*, \pi^{(k)})) = 1 \mid \pi_1 = \pi^*]$ by (\ref{eq:diff_s01_next}) we get
\begin{align}
\label{eq:pr_d_succ_0}
  \underset{r}{\Pr}[\Gamma_V^{(g)}(D(x^*, r)) = 1 \mid \pi_1 = \pi^*] &=
  \frac{\underset{r}{\Pr}[\Gamma_V^{(g)}(D(x^*, r)) = 1 \mid \pi_1 = \pi^*] \underset{\pi^{(k)}}{\Pr}[G_{\pi} \mid \pi_1 = \pi^*]} {\underset{\mu_{\delta}^{k}}{\Pr}[G_{\mu}]} \notag\\
  & \IndI - \frac{\underset{r}{\Pr}[\Gamma_V^{(g)}(D(x^*, r)) = 1 \mid \pi_1 = \pi^*](S_{\pi^*,1} - S_{\pi^*,0})}{\underset{\mu_{\delta}^{k}}{\Pr}[G_{\mu}]}
\end{align}
%
% \begin{todo}
%   \textbf{TODO:} Define $c_1, \dots, c_k$ correctly from the paper it is not known whether it is in the iteration or the final one
% \end{todo}
If output of circuit $D(x^*,r) \neq \bot$ then we denote $c_i := \Gamma_V^{i}(q, y_i)$.
We can write the first summand of (\ref{eq:pr_d_succ_0}) as
\begin{align}
  &\underset{r}{\Pr}[\Gamma_V^{(g)}(D(x^*,r)) = 1 \mid \pi_1 = \pi^*] \underset{\pi^{(k)}}{\Pr}[G_{\pi} \mid \pi_1 = \pi^*] = \notag\\
  &\IndI \underset{r}{\Pr}[D(x^*,r) \neq \bot \mid \pi_1 = \pi^*]
  \underset{\pi^{(k)}}{\Pr}[c_1 = 1 \mid G_{\pi}, \pi_1 = \pi^*]
  \underset{\pi^{(k)}}{\Pr}[G_{\pi} \mid \pi_1 = \pi^*]
\end{align}
where we make use of the fact that the event $G_{\pi}$ implies $D(x^*, r) \neq \bot$.
We consider two cases.
If $\underset{\pi^{k}}{\Pr}[g(1, c_2, \dots, c_k) = 1 \land g(0, c_2, \dots,c_k ) = 0 \mid \pi_1 = \pi^*] \leq \frac{\epsilon}{6k}$ then also
\begin{align}
  \underset{\pi^{(k)}}{\Pr}[c_1 = 1 \mid G_{\pi}, \pi_1 = \pi^*] \underset{\pi^{(k)}}{\Pr}[G_{\pi} \mid \pi_1 = \pi^*] \leq \frac{\epsilon}{6k}
\end{align}
and in the case when $\underset{\pi^{k}}{\Pr}[g(1, c_2, \dots, c_k) = 1 \land g(0, c_2, \dots,c_k ) = 0] > \frac{\epsilon}{6k}$ then circuit $D$ outputs $\bot$
only if it fails in all $\frac{6k}{\epsilon} \log(\frac{6k}{\epsilon})$ iterations to find $\pi^{(k)}$ such that $g(1, c_2, \dots, c_k) = 1 \land g(0, c_2, \dots, c_k) = 0$
which happens with probability
\begin{align}
\underset{r}{\Pr}[D(x^*,r) = \bot \mid \pi_1 = \pi^*] \leq (1 - \frac{\epsilon}{6k})^{\frac{6k}{\epsilon}\log(\frac{\epsilon}{6k})} \leq \frac{\epsilon}{6k}.
\end{align}
We conclude that in both cases we have
\begin{align}
  &\underset{r}{\Pr}[D(x^*,r) \neq \bot \mid \pi_1 = \pi^*]
  \underset{\pi^{(k)}}{\Pr}[c_1 = 1 \mid G_{\pi}, \pi_1 = \pi^*]
  \underset{\pi^{(k)}}{\Pr}[G_{\pi} \mid \pi_1 = \pi^*] \notag\\
  &\IndII \geq \underset{\pi^{(k)}}{\Pr}[c_1 = 1 \mid G_{\pi}, \pi_1 = \pi^*]\underset{\pi^{(k)}}{\Pr}[G_{\pi} \mid \pi_1 = \pi^*] - \frac{\epsilon}{6k}
\end{align}
Using definition of conditional probability we get
\begin{align*}
  &\underset{r}{\Pr}[D(x^*,r) \neq \bot \mid \pi_1 = \pi^*]
  \underset{\pi^{(k)}}{\Pr}[c_1 = 1 \mid G_{\pi}, \pi_1 = \pi^*]
  \underset{\pi^{(k)}}{\Pr}[G_{\pi} \mid \pi_1 = \pi^*] \notag\\
  &\IndII = \underset{\pi^{(k)}}{\Pr}[c_1 = 1 \land g(1, c_2,\dots, c_k) = 1 \land g(0, c_2, \dots, c_k) = 0 \mid \pi_1 = \pi^*] - \frac{\epsilon}{6k} \\
  &\IndII = \underset{\pi^{(k)}}{\Pr}[g(c_1, c_2,\dots, c_k) = 1 \land g(0, c_2, \dots, c_k) = 0 \mid \pi_1 = \pi^*] - \frac{\epsilon}{6k} \\
  &\IndII = \underset{\pi^{(k)}}{\Pr}[g(c_1, c_2,\dots, c_k) = 1 \mid \pi_1 = \pi^*] -  \underset{\pi^{(k)}}{\Pr}[g(0, c_2, \dots, c_k) = 0 \mid \pi_1 = \pi^*] - \frac{\epsilon}{6k} \\
\end{align*}
and finally by (\ref{eq:s_pi_b})
\begin{align}
  &\underset{r}{\Pr}[D(x^*,r) \neq \bot \mid \pi_1 = \pi^*]
  \underset{\pi^{(k)}}{\Pr}[c_1 = 1 \mid G_{\pi}, \pi_1 = \pi^*]
  \underset{\pi^{(k)}}{\Pr}[G_{\pi} \mid \pi_1 = \pi^*] \notag\\
  &\IndII = \underset{\pi^{(k)}}{\Pr}[g(c_1, c_2,\dots, c_k) = 1 \mid \pi_1 = \pi^*] -  \underset{\mu_{\delta}^{(k)}}{\Pr}[g(0, \mu_2, \dots, \mu_k) = 0 \mid \pi_1 = \pi^*]  - S_{\pi^*,0} - \frac{\epsilon}{6k}.
\end{align}
We insert this result into equation (\ref{eq:pr_d_succ_0}) to get
\begin{align}
\label{eq:pr_d_succ_1}
  &\underset{r,\pi}{\Pr}[D(x,r) = 1] = \mathbb{E_{\pi}}[\underset{r}{\Pr}[D(x,r) = 1 \mid \pi_1 = \pi^*]] \notag\\
&\IndI = \mathbb{E}_{\pi}\left[\frac{{\Pr}_{\pi^{(k)}}[g(c) = 1 \mid \pi_1 = \pi^*] -
{\Pr}_{\mu_{\delta}^{(k)}}[g(0, \mu_2, \dots, \mu_k) = 0 \mid \pi_1 = \pi^*] - \frac{\epsilon}{6k}} {{\Pr}_{\mu_{\delta}^{k}}[G_{\mu}]}\right] \notag\\
&\IndII - \mathbb{E}_{\pi}\left[\frac{
  S_{\pi^*,0} + \Pr_r [\Gamma_V^{(g)}(D(x^*,r)) = 1 \mid \pi_1 = \pi^*](S_{\pi^*,1} - S_{\pi^*,0})}
{{\Pr}_{\mu_{\delta}^{k}}[G_{\mu}]}\right]
\end{align}
For the second summand we want to show first that almost all estimates all low if we do not recurse.
Let assume that
\begin{align}
\underset{\pi}{\Pr}\left[\left(S_{\pi,0} \leq (1 - \frac{1}{2k})\epsilon\right) \land \left( S_{\pi,1} \leq (1-\frac{1}{2k})\epsilon\right)\right] < 1 - \frac{\epsilon}{6k},
\end{align}
then the algorithm would recurse almost surely.
Therefore, under the assumption that we do not recurse, we have almost surely
\begin{align}
\underset{\pi}{\Pr}\left[\left(S_{\pi,0} \leq (1 - \frac{1}{2k})\epsilon\right) \land \left( S_{\pi,1} \leq (1-\frac{1}{2k})\epsilon\right)\right] \geq 1 - \frac{\epsilon}{6k}.
\end{align}
Let us define a set
\begin{align}
  \X = \left\{ \pi :  \left(S_{\pi,0} \leq (1 - \frac{1}{2k})\epsilon\right) \land \left( S_{\pi,1} \leq (1-\frac{1}{2k})\epsilon \right) \right\}
\end{align}
and the complement of this set $\X^c$.
We bound the second summand in (\ref{eq:pr_d_succ_1})
\begin{align}
&\mathbb{E}_{\pi}\left[ S_{\pi^*,0} + \Pr_r [\Gamma_V^{(g)}(D(x^*,r)) = 1 \mid \pi_1 = \pi^*](S_{\pi^*,1} - S_{\pi^*,0}) \right] \notag\\
&\IndII = \mathbb{E}_{\pi \in \X^c}\left[ S_{\pi^*,0} + \Pr_r [\Gamma_V^{(g)}(D(x^*,r)) = 1 \mid \pi = \pi^*](S_{\pi^*,1} - S_{\pi^*,0}) \right] \notag\\
&\IndIII +  \mathbb{E}_{\pi \in \X}\left[ S_{\pi^*,0} + \Pr_r [\Gamma_V^{(g)}(D(x^*,r)) = 1 \mid \pi = \pi^*](S_{\pi^*,1} - S_{\pi^*,0}) \right] \\
&\IndII \leq \frac{\epsilon}{6k} + \mathbb{E}_{\pi \in \X^c}\left[ S_{\pi^*,0} + \Pr_r [\Gamma_V^{(g)}(D(x^*,r)) = 1 \mid \pi = \pi^*]((1 - \frac{1}{2k})\epsilon - S_{\pi^*,0}) \right] \\
&\IndII \leq \frac{\epsilon}{6k} + 1 - \frac{\epsilon}{2k} = 1 - \frac{\epsilon}{3k}
\end{align}
Finally, we insert this result into equation (\ref{eq:pr_d_succ_1}) and make use of the fact
\begin{align*}
\underset{}{\Pr}[g(u) = 1] &= \underset{}{\Pr}[(g(0, \mu_2, \dots, \mu_k) = 1) \lor ( g(1,\mu_2, \dots, \mu_k) = 1 \land g(0, \mu_2, \dots, \mu_k) = 0 \land \mu_1 = 1)] \notag\\
&= \Pr[g(0,\mu_2, \dots,\mu_k) = 1] + \Pr[g(1,\mu_2,\dots,\mu_k) = 1 \land g(0, \mu_2, \dots, \mu_k) = 0] \Pr[\mu_1 = 1]
\end{align*}
which yields
\begin{align*}
  \underset{r,\pi}{\Pr}[D(x,r) = 1]
&\geq \mathbb{E}_{\pi}\left[\frac{{\Pr}_{\pi^{(k)}}[g(c) = 1 \mid \pi_1 = \pi^*] -
{\Pr}_{\mu_{\delta}^{(k)}}[g(0, \mu_2, \dots, \mu_k) = 0] - (1 - \frac{1}{6k})\epsilon} {{\Pr}_{\mu_{\delta}^{k}}[G_{\mu}]}\right] \notag
 \end{align*}
 Using the assumptions of Lemma \ref{lemma:sec_amp_for_p_hash}, we get
 \begin{align*}
   \underset{r,\pi}{\Pr}[D(x,r) = 1]
 &\geq \frac{ {\Pr}_{\mu_{\delta}^{(k)}}[g(\mu) = 1] + \epsilon +
 \Pr_{\mu_{\delta}^{(k)}}[g(0, \mu_2, \dots, \mu_k) = 0] - (1 - \frac{1}{6k})\epsilon}
 {\Pr_{\mu_{\delta}^{k}}[G_{\mu}]} \notag\\
 &\geq \frac{\epsilon +
\delta\Pr_{\mu_{\delta}^{(k)}}[G_{\mu}] - (1 - \frac{1}{6k})\epsilon}
{\Pr_{\mu_{\delta}^{k}}[G_{\mu}]} \geq \delta + \frac{\epsilon}{6k}
\end{align*}

%%% Local Variables:
%%% mode: latex
%%% TeX-master: "../master"
%%% End:
