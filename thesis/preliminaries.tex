% \begin{todo}
%   \textbf{TODO:} Define probabilistic relation
% \end{todo}
%
% \begin{todo}
%   \textbf{TODO:} Define x \leftarrow D where D is prob. distribution
% \end{todo}
In this chapter we set up the notation and definitions used in the thesis.
\textbf{Functions.}
We write $\p(\alpha_1, \dots, \alpha_n)$ to denote a polynomial on variables $(\alpha_1, \dots, \alpha_n) \in \R^n$.
We call a function $f: \N \rightarrow \R$ \textit{negligible} if for every $\p(n)$
there exists $n_0 \in \N$ such that for all $n \in \N : n > n_0$ we have
\begin{align*}
f(n) < \frac{1}{\p(n)}.
\end{align*}
On the other hand, we say that a function $f: \N \rightarrow \R$ is \textit{non-negligible} if
there exists $\p(n)$ such that for some $n_0 \in \N$ and for all $n~\in~\N~:~n~>~n_0$ we have
\begin{align*}
  f(n) \geq \frac{1}{\p(n)}.
\end{align*}
We note that it is possible that a function $f:\N \rightarrow \R$ is neither negligible nor non-negligible.

We say that a function $f$ is \textit{efficiently computable} if there exists a polynomial time algorithm computing $f$.

\textbf{Algorithms, Bitstrings, and Circuits.}
We denote Boolean circuits using capital letters from the Greek or English alphabet.
We define a \textit{probabilistic circuit} as a Boolean circuit $C_{m,n} : \{0,1\}^{m} \times \{0,1\}^{n} \rightarrow \{0,1\}^{*}$ and
write $C_{m,n}(x;\rho)$ to denote a probabilistic circuit taking as input $x \in \{0,1\}^{m}$ and the randomness $\rho \in \{0,1\}^{n}$.
If a probabilistic circuit takes as input only the randomness, we slightly abuse the notation and write $C_{n}(r)$.
We make sure that it is clear from the context that probabilistic circuits that take as input only the randomness
are not confused with deterministic Boolean circuits. We use $\{C_n\}_{n \in \N}$ to denote a family of probabilistic circuits.
For a (probabilistic) circuit $C$ we write $\mathit{Size}(C)$ to denote the total number of vertices of $C$.
A \textit{family of (probabilistic) polynomial size circuits} is a family of (probabilistic) circuits where
the size of a circuit is polynomial in the number of input vertices.

Sometimes we slightly abuse the notation and write that a circuit $C$ takes as input $x \in \{0,1\}^{*}$ where
the length of $x$ is naturally bounded by the $Size(C)$ and $x$ might be padded accordingly if the length of input is shorter.

Sometimes we talk about interactive protocols consisting of two phases.
In this settings we define a \textit{two-phase circuit} $C := (C_1, C_2)$ as a circuit
where in the first phase a circuit $C_1$ is used and in the second phase a circuit $C_2$.
If $C_1$ and $C_2$ are probabilistic circuits we write $C(\delta) := (C_1, C_2)(\delta)$
to denote that in both phases $C_1$ and $C_2$ use the same randomness $\delta \in \{0,1\}^{*}$.

% It is well known \cite{Arora:2009:CCM:1540612} that a probabilistic polynomial time algorithm can be represented as a circuit of polynomial size.
% Moreover, it can be computed in polynomial time and logarithmic space.
%Therefore, whenever we state a theorem about circuits it can be also generalized for the polynomial time algorithms.

We write $\mathit{Time}(A)$ to denote the number of steps it takes to execute
an algorithm $A$ as a function of the length of input $A$ takes.
We say that $A$ runs in the \textit{polynomial time} if there exists $p(n)$
such that $\mathit{Time}(A)$ is bounded by $p(n)$ where $n$ denotes the length of the input that $A$ takes.

Similarly as for a probabilistic circuit we often write the randomness used by a probabilistic algorithm explicitly.
%as a bitstring provided as an auxiliary input.
% Exemplary, for a probabilistic algorithm $B$ we write $B(x;r)$
% where $x$ is the input taken by $B$ and $r$ denotes randomness taken as auxiliary input.

We denote a tuple $(x_1, \dotsc, x_l)$ by $x^{(l)}$. The $i$-th element of $x^{(l)}$ is denote by $x^{(l)}_i$
Furthermore, for tuples $x^{(l)}$, $y^{(k)}$ we use $x^{(l)} \circ y^{(k)}$ to denote the concatenation of $x^{(l)}$ and $y^{(k)}$ which results in
a tuple $(x_1, \dotsc, x_l, y_1, \dotsc, y_k)$.

% \begin{todo}
%   \textbf{TODO:} Define $\{0,1\}^{*}$ for circuits.
% \end{todo}

\textbf{Probabilities and distributions.}
For a finite set $\cR$ we write $r \xleftarrow{\$} \cR$ to denote that $r$ is chosen from $\cR$ uniformly at random.
For $\delta \in \R : 0 \leq \delta \leq 1$ we write $\mu_{\delta}$ to denote the Bernoulli distribution where outcome $1$ occurs with
probability $\delta$ and $0$ with probability $1-\delta$.
Moreover, we use $\mu_{\delta}^k$ to denote the probability distribution over $k$-tuples
where each element of a $k$-tuple is drawn independently according to $\mu_{\delta}$.
Finally, let $u \leftarrow \mu_{\delta}^k$ denote that a $k$-tuple $u$ is chosen according to $\mu_{\delta}^k$.

Let $(\Omega_n, \cF_n, \Pr)$ be a probability space and $n \in \N$.
Furthermore, let $E_n \in \cF_n$ denote an event whose probability depends on $n$.
We say that $E_n$ happens \textit{almost surely} or with \textit{high probability} if
there exists $p(n)$ such that $\Pr[E_n] \geq 1 - 2^{-n} \mathit{p}(n)$.
% \begin{todo}
%   \textbf{TODO:} Define non-negligible probability
% \end{todo}

\textbf{Interactive protocols.}
We are often interested in situations where two probabilistic circuits interact with
each other according to some protocol by means of messages representable by bitstrings.
Let $\{A_n\}_{n \in \N}$ and $\{B_n\}_{n \in \N}$ be families of circuits such that $A_n : \{0,1\}^{*} \rightarrow \{0,1\}^{*}$ and $B_n : \{0,1\}^{*} \rightarrow \{0,1\}^{*}$.
An \textit{interactive protocol} is defined by $\{A_n\}_{n \in \N}$ and $\{B_n\}_{n \in \N}$ where
for random bitstrings $\rho_A \in \{0,1\}^{n}$, $\rho_B \in \{0,1\}^{n}$ in the first round a message $m_0 := A_n(\rho_A)$ is sent and in the second round a message $m_1 := B_n(\rho_B, m_0)$.
In general in the $(2k\!-\!1)$-th round we have $m_{2k-2} := A_n(\rho_A, m_1, \cdots, m_{2k-3})$ and in the $2k$-th round $m_{2k-1} := B_n(\rho_B, m_1, \cdots, m_{2k-2})$.
The number of rounds is naturally bounded by the size of circuits $A_n$ and $B_n$.
The protocol execution between two probabilistic circuits $A$ and $B$ is denoted by $\langle A, B \rangle$.
The output of $A$ in the protocol execution is denoted by $\langle A, B \rangle_A$ and of $B$ by $\langle A, B \rangle_B$.
The sequence of all messages sent by $A$ and $B$ in the protocol execution is called a \textit{communication transcript} and
is denoted by $\langle A, B \rangle_{\mathit{trans}}$.

The time complexity of the protocol depends on the number of rounds needed to execute the protocol
and the size of $A_n$ and $B_n$. For example, a protocol runs in polynomial time if
the number of rounds and size of $A_n$ and $B_n$ are bounded by some $\p(n)$.

\textbf{Oracle algorithms.}
We use the notion of \textit{oracle circuits} following the standard definition in the literature \cite{Goldreich:2004:FCV:975541}.
If a circuit $A$ has oracle access to a circuit $B$, we write $A^B$. If additionally $B$ has oracle access to a circuit $C$,
we write $A^{B^C}$. However, to shorten the notation, we often write $A^{B}$ instead and make sure that it is clear from
the context which oracle is accessed by~$B$.

In many situations when studying the time complexity of algorithms with oracle access we count each oracle call as a single step.
We emphasize this by writing that an algorithm has a certain time complexity \textit{with oracle calls}.
On the other hand, in some settings we are interested in giving a more rigorous bound on the running time of an algorithm.
In these situations we compute the running time explicitly with regard to the time needed for accessing the oracle.
% \begin{todo}
%   \textbf{TODO:} Exponential factor in what?\\
%   \textbf{TODO:} Cite someone? \\
%   \textbf{TODO:} Define statistical distance etc.
% \end{todo}
\begin{definition}[Polynomial time samplable distribution.]
We say that a distribution is \textnormal{polynomial time samplable} if it can be
approximated by an algorithm running in time $\mathit{poly}(\log|\cD|, \log|\cR|)$
up to an exponential factor.
\end{definition}

\begin{definition}\textbf{(Pairwise independent family of efficient hash functions.)}
  \label{def:hash_pair_ind}
Let $\cD$ and $\cR$ be finite sets and $\cH$ be a family of functions mapping values from $\cD$ to values in $\cR$.
We say that $\cH$ is a \textnormal{family of pairwise independent efficient hash functions}
if $\cH$ has the following properties.

\textbf{Pairwise independent.} For all $x, y \in \mathcal{D}, x \neq y$ and for all $\alpha, \beta \in \cR$, it holds that
\begin{displaymath}
\underset{\hash \la0 \cH}{\Pr}[hash(x) = \alpha \mid hash(y) = \beta] = \frac{1}{|\cR|}.
\end{displaymath}

\textbf{Polynomial time samplable.} For every $\mathit{hash} \in \cH$ the function $\mathit{hash}$ is sampleable in time $\mathit{poly}(\log|\cD|, \log|\cR|)$.

\textbf{Efficiently computable.}
For every $hash \in \cH$ there exists an algorithm running in time $\mathit{poly}(\log|\cD|, \log|\cR|)$ which
on input $x \in \cD$ outputs $y \in \cR$ such that $y = hash(x)$.
\end{definition}

We note that the pairwise independence property is equivalent to
\begin{displaymath}
\underset{\hash \la0 \cH}{\Pr}[hash(x) = \alpha \land hash(y) = \beta] = \frac{1}{|\cR|^2}.
\end{displaymath}
It is well known \cite{Carter:1977:UCH:800105.803400} that there exists families of functions meeting the criteria stated in Definition \ref{def:hash_pair_ind}.

%%% Local Variables:
%%% mode: latex
%%% TeX-master: "master"
%%% End:
