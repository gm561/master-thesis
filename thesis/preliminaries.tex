% \begin{todo}
%   \textbf{TODO:} Define probabilistic relation
% \end{todo}
%
% \begin{todo}
%   \textbf{TODO:} Define x \leftarrow D where D is prob. distribution
% \end{todo}
In this chapter we set up the notation and introduce definitions used in the thesis.

% We denote Boolean circuits using capital letters from the Greek or English alphabet.
% Furthermore, we use calligraphic letters $\cA, \cB, \dotsc$ to denote sets and small letters to denote set elements.
% The randomness used by algorithms and circuits is denoted by small letters from the Greek alphabet $\rho, \delta, \dotsc$.

%TODO define tuples
\paragraph{Bitstrings and tuples.} We denote a tuple $(x_1, \dotsc, x_n)$ by $x^{(n)}$. The $i$-th element of $x^{(n)}$ is denote by $x^{(n)}_i$
Furthermore, for tuples $x^{(n)}$, $y^{(k)}$ we use $x^{(n)} \circ y^{(k)}$ to denote the concatenation of $x^{(n)}$ and $y^{(k)}$ which results in
a tuple $(x_1, \dotsc, x_l, y_1, \dotsc, y_k)$.
We denote a bitstring $(b_1, \dotsc, b_n) \in \{0,1\}^{n}$ as a $n$-tuple of bits.
% We write $x \in \{0,1\}^{*}$ to denote the bitstring $x$ of the arbitrary but bound length.
% If $x$ is the input of a circuit then it is naturally bounded by the size of a circuit
% whereas for algorithms it is bounded by the running time of an algorithm.

\paragraph{Functions.}
We write $\p(\alpha_1, \dots, \alpha_n)$ to denote a polynomial on variables $(\alpha_1, \dots, \alpha_n) \in \R^n$.
We call a function $f: \N \rightarrow \R$ \textit{negligible} if for every $\p(n)$
there exists $n_0 \in \N$ such that for all $n \in \N : n > n_0$ we have
\begin{align*}
f(n) < \frac{1}{\p(n)}.
\end{align*}
On the other hand, we say that a function $f: \N \rightarrow \R$ is \textit{non-negligible} if
there exists $\p(n)$ such that for some $n_0 \in \N$ and for all $n~\in~\N~:~n~>~n_0$ we have
\begin{align*}
  f(n) \geq \frac{1}{\p(n)}.
\end{align*}
We note that it is possible that a function $f:\N \rightarrow \R$ is neither negligible nor non-negligible.

We say that a function $f$ is \textit{efficiently computable} if there exists a polynomial time algorithm computing $f$.

For a function $f(n): \R \rightarrow \R$ we write $f(n) = \poly(n)$ to denote that there exists a polynomial $p(n)$ that
bounds $f(n)$ for $n$ big enough.

\begin{definition}[Binary monotone function]
For $n \in \N$ we say that $g: \{0,1\}^n \rightarrow \{0,1\}$ is a \textit{binary monotone function}
if for any two bitstrings $(b_1, \dotsc, b_n) \in \{0,1\}^{n}$ and $(b_1', \dotsc, b_n') \in \{0,1\}^{n}$
and sets $\cI := \{i \in \{1,\dotsc, n\}: b_i = 1\}$ and $\cI' := \{i \in \{1,\dotsc, n\} : b_i' = 1\}$ it holds
that if $\cI \subseteq \cI'$ and $g(b_1, \dotsc, b_n) = 1$ then $g(b_1', \dotsc, b_n') = 1$.
\end{definition}

\paragraph{Algorithms and Circuits.}
We define a \textit{probabilistic circuit} as a Boolean circuit $C_{m,n} : \{0,1\}^{m} \times \{0,1\}^{n} \rightarrow \{0,1\}^{*}$ and
write $C_{m,n}(x;\rho)$ to denote a probabilistic circuit that takes as input $x \in \{0,1\}^{m}$ and the randomness $\rho \in \{0,1\}^{n}$.
If a probabilistic circuit takes as input only the randomness, we slightly abuse the notation and write $C_{n}(\rho)$.
We make sure that it is clear from the context that probabilistic circuits that take as input only the randomness
are not confused with deterministic Boolean circuits. We use $\{C_n\}_{n \in \N}$ to denote a family of probabilistic circuits.
For a (probabilistic) circuit $C$ we write $\mathit{Size}(C)$ to denote the total number of vertices of $C$.
W define a \textit{family of (probabilistic) polynomial size circuits} as a family of (probabilistic) circuits where
the size of a circuit is polynomial in the number of input vertices.

Sometimes we talk about interactive protocols consisting of two phases where two probabilistic circuits interact with each other.
In this settings we define a \textit{two-phase circuit} $C := (C_1, C_2)$ as a circuit
where in the first phase a circuit $C_1$ is used and in the second phase a circuit $C_2$.
%If $C_1$ and $C_2$ are probabilistic circuits
We write $C(\delta) := (C_1, C_2)(\delta)$ to denote that $C_1$ and $C_2$ use the same randomness $\delta \in \{0,1\}^{*}$.

We write $\mathit{Time}(A)$ to denote the number of steps it takes to execute
an algorithm $A$ as a function of the length of input $A$ takes.
We say that $A$ runs in the \textit{polynomial time} if there exists $p(n)$
such that $\mathit{Time}(A)$ is bounded by $p(n)$ where $n \in \N$ denotes the length of the input that $A$ takes.

Similarly as for a probabilistic circuit we often write the randomness used by a probabilistic algorithm explicitly.
%as a bitstring provided as an auxiliary input.
% Exemplary, for a probabilistic algorithm $B$ we write $B(x;r)$
% where $x$ is the input taken by $B$ and $r$ denotes randomness taken as auxiliary input.

% \begin{todo}
%   \textbf{TODO:} Define $\{0,1\}^{*}$ for circuits.
% \end{todo}

\textbf{Probabilities and distributions.}
For a finite set $\cR$ we write $r \xleftarrow{\$} \cR$ to denote that $r$ is chosen from $\cR$ uniformly at random.
For $\delta \in \R : 0 \leq \delta \leq 1$ we write $\mu_{\delta}$ to denote the Bernoulli distribution where outcome $1$ occurs with
probability $\delta$ and $0$ with probability $1-\delta$.
Moreover, we use $\mu_{\delta}^k$ to denote the probability distribution over $k$-tuples
where each element of a $k$-tuple is drawn independently according to $\mu_{\delta}$.
Finally, let $u \leftarrow \mu_{\delta}^k$ denote that a $k$-tuple $u$ is chosen according to $\mu_{\delta}^k$.

% \begin{todo}
%   \textbf{TODO:} Distribution ensemble \\
% \end{todo}

Let $(\Omega_n, \cF_n, \Pr)$ be a probability space and $n \in \N$.
Furthermore, let $E_n \in \cF_n$ denote an event whose probability depends on $n$.
We say that $E_n$ happens \textit{almost surely} or with \textit{high probability} if
there exists $n_0 \in \N$ such that for all $n \in \N : n > n_0$ there exists $p(n)$ such that $\Pr[E_n] \geq 1 - 2^{-n} \mathit{p}(n)$.
% \begin{todo}
%   \textbf{TODO:} Define non-negligible probability
% \end{todo}

\textbf{Interactive protocols.}
We are often interested in situations where two probabilistic circuits interact with
each other according to some protocol by means of messages representable by bitstrings.
Let $\{A_n\}_{n \in \N}$ and $\{B_n\}_{n \in \N}$ be families of circuits such that $A_n : \{0,1\}^{*} \rightarrow \{0,1\}^{*}$
and $B_n : \{0,1\}^{*} \rightarrow \{0,1\}^{*}$\footnote{We represent the input of $A_n$ as some tuple that is encoded into a bitstring.
The same approach is used for the input of $B_n$.}
An \textit{interactive protocol} is defined by $\{A_n\}_{n \in \N}$ and $\{B_n\}_{n \in \N}$ where
for random bitstrings $\rho_A \in \{0,1\}^{n}$, $\rho_B \in \{0,1\}^{n}$ in the first round a message $m_0 := A_n(\rho_A)$ is sent and in the second round a message $m_1 := B_n(\rho_B, m_0)$.
In general in the $(2k\!-\!1)$-th round we have $m_{2k-2} := A_n(\rho_A, m_1, \cdots, m_{2k-3})$ and in the $2k$-th round $m_{2k-1} := B_n(\rho_B, m_1, \cdots, m_{2k-2})$.
The number of rounds of interaction is naturally bounded by the size of circuits $A_n$ and $B_n$.
The protocol execution between two probabilistic circuits $A$ and $B$ is denoted by $\langle A, B \rangle$.
The output of $A$ in the protocol execution is denoted by $\langle A, B \rangle_A$ and of $B$ by $\langle A, B \rangle_B$.
The sequence of all messages sent by $A$ and $B$ in the protocol execution is called a \textit{communication transcript} and
is denoted by $\langle A, B \rangle_{\mathit{trans}}$.
The time complexity of the protocol depends on the size of $A_n$ and $B_n$.
For example, we say that a protocol runs in the polynomial time if the size of $A_n$ and $B_n$ are bounded by some $\p(n)$.

\textbf{Oracle algorithms.}
We use the notion of \textit{oracle circuits} following the standard definition in the literature \cite{Goldreich:2004:FCV:975541}.
If a circuit $A$ has oracle access to a circuit $B$, we write $A^B$. If additionally $B$ has oracle access to a circuit $C$,
we write $A^{B^C}$. However, to shorten the notation, we often write $A^{B}$ instead and make sure that it is clear from
the context which oracle is accessed by~$B$.

In many situations when studying the time complexity of algorithms with oracle access we count each oracle call as a single step.
We emphasize this by writing that an algorithm has a certain time complexity \textit{with oracle calls}.
On the other hand, in some settings we are interested in giving a more rigorous bound on the running time of an algorithm.
In these situations we compute the running time explicitly with regard to the time needed for accessing the oracle.
% \begin{todo}
%   \textbf{TODO:} Exponential factor in what?\\
%   \textbf{TODO:} Cite someone? \\
%   \textbf{TODO:} Define statistical distance etc.
% \end{todo}
% \begin{definition}[Polynomial time samplable distribution.]
% We say that a distribution is \textnormal{polynomial time samplable} if it can be
% approximated by an algorithm running in time $\mathit{poly}(\log|\cD|, \log|\cR|)$
% up to an exponential factor.
% \end{definition}

\begin{definition}[Pairwise independent family of efficient hash functions]
  \label{def:hash_pair_ind}
Let $\cD$ and $\cR$ be finite sets and $\cH$ be a family of functions mapping values from $\cD$ to values in $\cR$.
We say that $\cH$ is a \textnormal{family of pairwise independent efficient hash functions}
if $\cH$ has the following properties.

\textbf{Pairwise independent.} For all $x, y \in \mathcal{D}, x \neq y$ and for all $\alpha, \beta \in \cR$, it holds that
\begin{displaymath}
\underset{\hash \la0 \cH}{\Pr}[hash(x) = \alpha \mid hash(y) = \beta] = \frac{1}{|\cR|}.
\end{displaymath}

\textbf{Polynomial time samplable.} For every $\mathit{hash} \in \cH$ the function $\mathit{hash}$ is samplable in time $\poly(\log|\cD|, \log|\cR|)$.

\textbf{Efficiently computable.}
For every $hash \in \cH$ there exists an algorithm running in time $\mathit{poly}(\log|\cD|, \log|\cR|)$ which
on input $x \in \cD$ outputs $y \in \cR$ such that $y = hash(x)$.
\end{definition}

We note that the pairwise independence property is equivalent to
\begin{displaymath}
\underset{\hash \la0 \cH}{\Pr}[hash(x) = \alpha \land hash(y) = \beta] = \frac{1}{|\cR|^2}.
\end{displaymath}
It is well known \cite{Carter:1977:UCH:800105.803400} that there exists families of functions meeting the criteria stated in Definition \ref{def:hash_pair_ind}.

%%% Local Variables:
%%% mode: latex
%%% TeX-master: "master"
%%% End:
