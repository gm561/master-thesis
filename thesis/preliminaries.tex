%
% \begin{todo}
%   \textbf{TODO:} Define probabilistic relation
% \end{todo}
%
% \begin{todo}
%   \textbf{TODO:} Define x \leftarrow D where D is prob. distribution
% \end{todo}
In this chapter we set up notation and terminology used in the Thesis.
\section{Notation and Definitions}
\textbf{(Functions)}
We write $\mathit{poly}(\alpha_1, \dots, \alpha_n)$ to denote a polynomial on variables $\alpha_1, \dots, \alpha_n \in \R^n$.
We call a function $f: \N \rightarrow \R$ \textit{negligible} if for every polynomial $\poly(n)$
there exists $n_0 \in \N$ such that for all $n \in \N : n > n_0$ the following holds
\begin{align*}
f(n) < \frac{1}{\poly(n)}.
\end{align*}
On the other hand, we say that a function $f: \N \rightarrow \R$ is \textit{non-negligible} if
there exists a polynomial $\poly(n)$ such that for some $n_0 \in \N$ and for all $n~\in~\N~:~n~>~n_0$ we have
\begin{align*}
  f(n) \geq \frac{1}{\poly(n)}.
\end{align*}
We note that it is possible that a function $f:\N \rightarrow \R$ is neither negligible nor non-negligible.

We say that a function $f$ is \textit{efficiently computable} if there exists a polynomial time algorithm computing $f$.

\textbf{(Algorithms, Bitstrings, and Circuits)}
We denote Boolean circuits using capital letters from the Greek or English alphabet.
We define a \textit{probabilistic circuit} as a Boolean circuit $C_{m,n} : \{0,1\}^{m} \times \{0,1\}^{n} \rightarrow \{0,1\}^{*}$ and
write $C_{m,n}(x;r)$ to denote a probabilistic circuit taking as input  $x \in \{0,1\}^{m}$ and as auxiliary input $r \in \{0,1\}^{n}$.
If a probabilistic circuit does not take any input, we slightly abuse notation and write $C_{n}(r)$.
Similarly, we use $\{C_n\}_{n \in \N}$ to denote a family of probabilistic circuits that takes only auxiliary input.
We make sure that it is clear from the context that probabilistic circuits (families of probabilistic circuits) with only auxiliary input
are not confused with deterministic Boolean circuits (families of Boolean circuits respectively).
For a (probabilistic) circuit $C$ we write $\mathit{Size}(C)$ to denote the total number of vertices of $C$.
A \textit{(probabilistic) polynomial size circuit} is a (probabilistic) circuit of size polynomial in the number of input vertices
(including auxiliary input vertices if the circuit is probabilistic).
We define a \textit{two phase circuit} $C := (C_1, C_2)$ as a circuit where in the first phase a circuit $C_1$ is used and in the second phase a circuit $C_2$.\footnote{
Where what a \textit{phase} means should be clear from the context in which $C$ is used.}
If $C_1$ and $C_2$ are probabilistic circuits we write $C(\delta) := (C_1, C_2)(\delta)$ to denote that in both phases $C_1$ and $C_2$ take
as auxiliary input the same bitstring $\delta$.

% It is well known \cite{Arora:2009:CCM:1540612} that a probabilistic polynomial time algorithm can be represented as a circuit of polynomial size.
% Moreover, it can be computed in polynomial time and logarithmic space.
%Therefore, whenever we state a theorem about circuits it can be also generalized for the polynomial time algorithms.

For an algorithm $A$ we write $\mathit{Time}(A)$ to denote the number of steps it takes to execute $A$.
We say that $A$ runs in \textit{polynomial time} if $\mathit{Time}(A)$ is bounded by some $poly(|x|)$ where $|x|$ denotes the length of the input that $A$ takes.

Similarly as for probabilistic circuits we often write randomness used by a probabilistic algorithm explicitly
as a bitstring provided as an auxiliary input.
% Exemplary, for a probabilistic algorithm $B$ we write $B(x;r)$
% where $x$ is the input taken by $B$ and $r$ denotes randomness taken as auxiliary input.

We denote a tuple $(x_1, \dotsc, x_l)$ by $x^{(l)}$.
The $i$-th element of $x^{(l)}$ is denote by $x^{(l)}_i$
Furthermore, for tuples $x^{(l)}$, $y^{(k)}$ we use $x^{(l)} \circ y^{(k)}$ to denote the concatenation of $x^{(l)}$ and $y^{(k)}$ which results in
a tuple $(x_1, \dotsc, x_l, y_1, \dotsc, y_k)$.

\textbf{(Probabilities and distributions)}
For a finite set $\cR$ we write $r \xleftarrow{\$} \cR$ to denote that $r$ is chosen from $\cR$ uniformly at random.
For $\delta \in \R : 0 \leq \delta \leq 1$ we write $\mu_{\delta}$ to denote the Bernoulli distribution where outcome $1$ occurs with
probability $\delta$ and $0$ with probability $1-\delta$.
Moreover, we use $\mu_{\delta}^k$ to denote the probability distribution over $k$-tuples
where each element of a $k$-tuple is drawn independently according to $\mu_{\delta}$.
Finally, let $u \leftarrow \mu_{\delta}^k$ denote that a $k$-tuple $u$ is chosen according to $\mu_{\delta}^k$.

Let $(\Omega_n, \cF_n, \Pr)$ be a probability space and $n \in \N$.
Furthermore, let $E_n \in \cF_n$ denote an event that probability depends on $n$.
We say that $E_n$ happens \textit{almost surely} or with \textit{high probability} if $\Pr[E_n] \geq 1 - 2^{-n} \mathit{poly}(n)$.
% \begin{todo}
%   \textbf{TODO:} Define non-negligible probability
% \end{todo}

\textbf{(Interactive protocols)}
We are often interested in situations where two probabilistic circuits interact with each other according to some protocol.
We limit ourselves to cases where messages are representable by bitstrings.
Let $\{A_n\}_{n \in \N}$ and $\{B_n\}_{n \in \N}$ be families of circuits such that $A_n : \{0,1\}^{*} \rightarrow \{0,1\}^{*}$ and $B_n : \{0,1\}^{*} \rightarrow \{0,1\}^{*}$.
An \textit{interactive protocol} is defined by $\{A_n\}_{n \in \N}$ and $\{B_n\}_{n \in \N}$ where
for random bitstrings $\rho_A \in \{0,1\}^{n}$, $\rho_B \in \{0,1\}^{n}$ in the first round $m_0 := A_n(\rho_A)$ and in the second round $m_1 := B_n(\rho_B, m_1)$.
In general in the $(2k\!-\!1)$-th round we have $m_{2k-2} := A_n(\rho_A, m_1, \cdots, m_{2k-3})$ and in the $2k$-th round $m_{2k-1} := B_n(\rho_B, m_1, \cdots, m_{2k-2})$.
The protocol execution between two probabilistic circuits $A$ and $B$ is denoted by $\langle A, B \rangle$.
The output of $A$ in the protocol execution is denoted by $\langle A, B \rangle_A$ and of $B$ by $\langle A, B \rangle_B$.
A~sequence of all messages send by $A$ and $B$ in the protocol execution is called a \textit{communication transcript} and
is denoted by $\langle A, B \rangle_{\mathit{trans}}$.

The time complexity of the protocol depends on the number of rounds needed to execute the protocol
and the number of steps required to evaluate $A_n$ and $B_n$.
Exemplary, the protocol runs in polynomial time if the number of rounds and time needed to evaluate $A_n$ and $B_n$ is bounded by some $\poly(n)$.

\textbf{(Oracle algorithms)}
We use notion of \textit{oracle circuits} following the standard definition included in the literature \cite{Goldreich:2004:FCV:975541}.
If a circuit $A$ gains oracle access to a circuit $B$, we write $A^{B}$. If additionally $B$ gains oracle access to a circuit $C$,
we write $A^{B^C}$. However, to shorten notation, we often write $A^{B}$ instead and make sure that it is clear from the context which oracle is accessed by~$B$.

In many situations when studying time complexity of algorithms with oracle access we count oracle call as a single step.
We emphasize this by writing that an algorithm has a certain time complexity \textit{with oracle calls}.
On the other hand, in some settings we are interested in giving a more rigorous bounds on running time of an algorithm.
In these situations we compute running time explicitly with regard to time needed for accessing oracle.
% \begin{todo}
%   \textbf{TODO:} Exponential factor in what?\\
%   \textbf{TODO:} Cite someone? \\
%   \textbf{TODO:} Define statistical distance etc.
% \end{todo}
\begin{definition}[Polynomial time sampleable distribution]
We say that a distribution is \textnormal{polynomial time sampleable} if it can be approximated by an algorithm running in time $\mathit{poly}(\log|\cD|, \log|\cR|)$
up to an exponential factor.
\end{definition}

\begin{definition}[Pairwise independent family of efficient hash functions]
  \label{def:hash_pair_ind}
Let $\cD$ and $\cR$ be finite sets and $\cH$ be a family of functions mapping values from $\cD$ to values in $\cR$.
We say that $\cH$ is a \textnormal{family of pairwise independent efficient hash functions}
if $\cH$ has the following properties.

\textbf{(Pairwise independent)} For $\forall x \neq y \in \mathcal{D}$ and $\forall \alpha, \beta \in \cR$, it holds
\begin{displaymath}
\underset{\hash \la0 \cH}{\Pr}[hash(x) = \alpha \mid hash(y) = \beta] = \frac{1}{|\cR|}.
\end{displaymath}

\textbf{(Polynomial time sampleable)} For every $\mathit{hash} \in \cH$ the function $\mathit{hash}$ is sampleable in time $\mathit{poly}(\log|\cD|, \log|\cR|)$.

\textbf{(Efficiently computable)}
For every $hash \in \cH$ there exists an algorithm running in time $\mathit{poly}(\log|\cD|, \log|\cR|)$ which
on input $x \in \cD$ outputs $y \in \cR$ such that $y = hash(x)$.
\end{definition}

We note that the pairwise independence property is equivalent to
\begin{displaymath}
\underset{\hash \la0 \cH}{\Pr}[hash(x) = \alpha \land hash(y) = \beta] = \frac{1}{|\cR|^2}.
\end{displaymath}
It is well know \cite{Carter:1977:UCH:800105.803400} that there exists families of functions meeting the criteria stated in Definition \ref{def:hash_pair_ind}.

%%% Local Variables:
%%% mode: latex
%%% TeX-master: "master"
%%% End:
